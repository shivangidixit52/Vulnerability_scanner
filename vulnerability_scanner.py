import requests
from bs4 import BeautifulSoup
import re
import socket

# Function to get the title of a webpage
def get_title(url):
    data = read_contents(url).decode('utf-8')
    title_pattern = re.compile(r'<title[^>]*>(.*?)<\/title>', re.I | re.S)
    title = title_pattern.search(data)
    return title.group(1) if title else None

# Function to detect the web server of a URL
def web_server(url):
    wsheaders = requests.head(url).headers
    ws = wsheaders.get('Server', '')
    if not ws:
        print('\033[91mCould Not Detect\033[0m')
    else:
        print('\033[92mServer:{}\033[0m'.format(ws))

# Function to detect if a website is behind Cloudflare
def cloudflare_detect(url):
    urlhh = 'http://api.hackertarget.com/httpheaders/?q={}'.format(url)
    resulthh = requests.get(urlhh).text.lower()
    if 'cloudflare' in resulthh:
        print('\033[91mcloudflareDetected\033[0m')
    else:
        print('\033[92mcloudflare Not Detected\033[0m')



# Function to fetch and display the contents of robots.txt
def robots_dot_txt(url):
    rbturl = '{}/robots.txt'.format(url)
    rbthandle = requests.get(rbturl)
    if rbthandle.status_code == 200:
        rbtcontent = rbthandle.text
        if not rbtcontent:
            print('Found But Empty!')
        else:
            print('\033[92mrobots.txt:Found\033[0m\n')
            print('\033[36m\n-------------[ contents ]----------------\033[0m\n')
            print(rbtcontent)
            print('\033[36m\n-----------[end of contents]-------------\033[0m')
    else:
        print('\033[91mCould NOT Find robots.txt!\033[0m')

# Function to get HTTP headers of a URL
def get_http_header(url):
    hdr = requests.head(url).headers
    for shdr, value in hdr.items():
        print(f'\n\033[92m[i]\033[0m {shdr}: {value}')
    print()

# Function to extract links from the source code of a webpage
def extract_links(url):
    ip = url.replace('https://', '')
    lwwww = ip.replace('www.', '')
    elsc = read_contents(url)
    eldom = BeautifulSoup(elsc, 'html.parser')
    links = eldom.find_all('a')
    return links


def read_contents(url):
    response = requests.get(url)
    if response.status_code == 200:
        return response.content
    else:
        return None


if __name__ == '__main__':
    url = input('Enter a URL: ')
    links = extract_links(url)
    for link in links:
        print(f'\033[92m[i]\033[0m {link["href"]}')

def check_open_ports(url):
    ports = [80, 443, 8080, 21, 22, 23]  # Example list of ports to check

    for port in ports:
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                s.settimeout(1)
                result = s.connect_ex((url, port))

                if result == 0:
                    print(f"Port {port} on {url} is open.")
                else:
                    print(f"Port {port} on {url} is closed.")
        except Exception as e:
            print(f"Error occurred while scanning port {port}: {e}")

# Print the title of the webpage
title = get_title(url)
if title:
    print('\n\033[92mTitle: {}\033[0m'.format(title))

# Print the web server of the webpage
web_server(url)

# Print if the webpage is behind Cloudflare
cloudflare_detect(url)


# Print the contents of robots.txt
robots_dot_txt(url)

# Print the HTTP headers of the webpage
get_http_header(url)


check_open_ports(url)

